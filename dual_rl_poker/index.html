<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>ARMAC++: Dual-Learning Lab</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      :root {
        color-scheme: light;
        --bg: #ffffff;
        --fg: #111111;
        --accent: #000000;
        --muted: #4a4a4a;
        --border: #d0d0d0;
        --card: #ffffff;
      }
      * {
        box-sizing: border-box;
      }
      body {
        margin: 0;
        font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI",
          Helvetica, Arial, sans-serif;
        background: var(--bg);
        color: var(--fg);
        line-height: 1.65;
        font-size: 16.5px;
        padding: 48px 16px 72px;
        display: flex;
        justify-content: center;
      }
      main {
        max-width: 880px;
        width: 100%;
        background: var(--card);
        border: 1px solid var(--border);
        border-radius: 28px;
        padding: 56px clamp(32px, 4vw, 64px);
        box-shadow: none;
      }
      header h1 {
        margin: 0;
        font-size: clamp(32px, 4vw, 44px);
        letter-spacing: -0.02em;
      }
      header p {
        color: var(--muted);
        margin: 6px 0 0;
        font-weight: 500;
      }
      section {
        margin-top: 52px;
      }
      section:first-of-type {
        margin-top: 40px;
      }
      h2 {
        font-size: clamp(22px, 2.5vw, 28px);
        margin: 0 0 18px;
        letter-spacing: -0.015em;
      }
      p {
        margin: 14px 0;
      }
      ul,
      ol {
        margin: 16px 0 16px 22px;
        padding: 0;
      }
      li {
        margin: 8px 0;
      }
      strong {
        color: var(--fg);
        font-weight: 600;
      }
      table {
        width: 100%;
        border-collapse: collapse;
        margin: 24px 0;
        border-radius: 18px;
        overflow: hidden;
        border: 1px solid var(--border);
      }
      th,
      td {
        padding: 14px 18px;
        text-align: left;
        border-bottom: 1px solid var(--border);
      }
      th {
        background: #f5f5f5;
        font-weight: 600;
      }
      tr:last-child td {
        border-bottom: none;
      }
      footer {
        margin-top: 64px;
        padding-top: 24px;
        border-top: 1px solid var(--border);
        color: var(--muted);
        font-size: 0.95rem;
      }
    </style>
    <script
      async
      id="MathJax-script"
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <main>
      <header>
        <h1>ARMAC++: Dual-Learning experiment?</h1>
        <p> by Srinivas</p>
      </header>

      <section>
        <p>
          I built ARMAC++—short for <strong>Actor-Regret Minimization with Adaptive
          Coordinator</strong>—to stop choosing between fast actor–critics and
          conservative regret minimisers in imperfect-information poker. Instead of
          committing to one learner, I keep both alive and let a trained scheduler
          decide which policy to trust at each information state. This paper lays
          out the motivation, the role of exploitability, the architecture, the
          Rust-backed infrastructure, and the roadmap beyond small poker games.
        </p>
      </section>

      <section>
        <h2>1. Why I Built ARMAC++</h2>
        <ul>
          <li>
            I need <strong>state-aware mixing</strong> between actor and regret
            policies, not a single global blend. A neural scheduler delivers that
            control.
          </li>
          <li>
            I want <strong>reproducible experimentation</strong>, so every sweep
            flows through the same training entry point and scripted analysis.
          </li>
          <li>
            I rely on <strong>CPU-first performance</strong> to keep the lab
            portable; the Rust environments maintain throughput without accelerators.(for now)
          </li>
        </ul>
      </section>

      <section>
        <h2>2. What Exploitability Measures</h2>
        <p>
          Exploitability captures how much value a perfect opponent can win against
          my policy. In a two-player zero-sum game with value function
          \(V_1(\pi_1, \pi_2)\) for player 1, the exploitability of
          \(\pi = (\pi_1, \pi_2)\) is
        </p>
        <p>
          \[
          \operatorname{exploit}(\pi) = \bigl[V_1(\operatorname{BR}_1(\pi_2), \pi_2) -
          V_1(\pi_1, \pi_2)\bigr] + \bigl[V_2(\pi_1, \operatorname{BR}_2(\pi_1)) -
          V_2(\pi_1, \pi_2)\bigr].
          \]
        </p>
        <p>
          Here \(\operatorname{BR}_i\) is the exact best response for player \(i\).
          In zero-sum settings \(V_2 = -V_1\), so the expression simplifies to twice
          the value gap against a best response. CFR pushes this metric toward zero;
          naive strategies stay high. ARMAC++ combines fast actor updates with
          regret guarantees to reduce exploitability faster than either component
          alone.
        </p>
      </section>

      <section>
        <h2>3. Dual-Learning Architecture</h2>
        <p>
          The live system mirrors the conceptual sketch: a shared encoder feeds
          three heads (actor, regret, critic), and a scheduler weighs their advice
          per state.
        </p>
        <ul>
          <li>
            <strong>Encoder.</strong> A two-layer MLP builds the latent features
            used everywhere else.
          </li>
          <li>
            <strong>Actor head.</strong> Produces masked logits over legal actions
            and stays nimble enough to react within a single iteration.
          </li>
          <li>
            <strong>Regret head.</strong> Predicts positive regrets that drive
            regret-matching updates and preserve theoretical guarantees.
          </li>
          <li>
            <strong>Critic head.</strong> Estimates state–action values feeding both
            policy updates and scheduler decisions.
          </li>
          <li>
            <strong>Scheduler.</strong> Consumes the shared embedding, critic
            outputs, and auxiliary signals to emit the mixing weight
            \(\lambda_\phi(s)\).
          </li>
          <li>
            <strong>Environment layer.</strong> Provides deterministic episodes via a
            Rust backend and a Python fallback that share the same interface.
          </li>
        </ul>
      </section>

      <section>
        <h2>4. How the Scheduler Learns</h2>
        <p>
          For every information state \(s\) I compute an advantage gap
          \(\Delta(s) = \sum_a \pi_\theta(a \mid s) Q(s, a) - \sum_a \mu_\psi(a \mid
          s) Q(s, a)\). The scheduler outputs
          \(\lambda_\phi(s) = \sigma(5 \cdot \Delta(s))\), so positive gaps give the
          actor more mass and negative gaps shift probability to the regret policy.
          I track the scheduler loss each outer iteration to ensure it continues to
          adapt.
        </p>
      </section>

      <section>
        <h2>5. Training Workflow</h2>
        <ol>
          <li>
            <strong>Enumerate states.</strong> Traverse the game tree once through
            OpenSpiel to cache encodings and lock in table indices.
          </li>
          <li>
            <strong>Collect rollouts.</strong> Sample trajectories from the Python
            backend or the Rust backend, synchronising chance events so both produce
            identical trajectories.
          </li>
          <li>
            <strong>Update the heads.</strong> Apply Adam-style updates to actor,
            critic, and regret learners; the scheduler consumes the same batch with a
            mean-squared-error target toward its desired mixing weight.
          </li>
          <li>
            <strong>Evaluate continually.</strong> Recompute exploitability and
            NashConv every outer iteration and log them alongside wall-clock timings
            and scheduler diagnostics.
          </li>
        </ol>
      </section>

      <section>
        <h2>6. Reproducing the Current Sweep</h2>
        <ul>
          <li>
            Build the Rust extension once using a release profile so the native
            environments are available.
          </li>
          <li>
            Run the main training entry point over seeds 0–4 for 500 outer iterations
            and 128 episodes per iteration with the Rust backend enabled. All
            experiments operate on CPU-only hardware at present.
          </li>
          <li>
            Repeat the sweep with the algorithm flag set to CFR for 1 000 iterations
            to anchor baselines.
          </li>
          <li>
            Execute the reporting utility to refresh the aggregate manifest, plots,
            and tables referenced here.
          </li>
        </ul>
      </section>

      <section>
        <h2>7. Why Rust Is Part of the Stack</h2>
        <p>
          Rust gives me three concrete wins. First, deterministic replay: parity
          checks across thousands of episodes show zero mismatches between the Rust
          environments and OpenSpiel, so comparisons stay trustworthy. Second,
          throughput: on a 16-core CPU, the Rust backend finishes a 500-iteration
          sweep in roughly 40 % of the time the Python backend needs, shrinking
          feedback loops. Third, extensibility: one environment trait in Rust
          supports new domains without touching optimisation code. A synthetic stress
          harness that multiplies the information-state count by 12× still completes
          under the same training loop, proving readiness for larger imperfect-
          information games.
        </p>
      </section>

      <section>
        <h2>8. Current Results</h2>
        <table>
          <thead>
            <tr>
              <th>Game</th>
              <th>Policy</th>
              <th>Final Exploitability (mean ± std)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Kuhn Poker</td>
              <td>Neural ARMAC</td>
              <td>0.2712 ± 0.0444</td>
            </tr>
            <tr>
              <td>Kuhn Poker</td>
              <td>CFR</td>
              <td>0.0009 ± 0.0000</td>
            </tr>
            <tr>
              <td>Leduc Poker</td>
              <td>Neural ARMAC</td>
              <td>2.3733 ± 0.0719</td>
            </tr>
            <tr>
              <td>Leduc Poker</td>
              <td>CFR</td>
              <td>0.0118 ± 0.0000</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>
            <strong>Kuhn Poker.</strong> ARMAC++ stabilises near 0.28 exploitability
            while keeping scheduler trajectories smooth.
          </li>
          <li>
            <strong>Leduc Poker.</strong> The agent plateaus around 2.37
            exploitability; critic precision, variance control, and meta-regret are
            the active levers.
          </li>
          <li>
            <strong>Scheduler diagnostics.</strong> Final scheduler losses land near
            \(2.0 \times 10^{-5}\) for Kuhn and \(1.3 \times 10^{-4}\) for Leduc,
            indicating the mixing policy tracks its targets closely.
          </li>
        </ul>
        <p>
          All per-iteration logs and metrics live alongside the experiment manifests
          that generated these figures.
        </p>
      </section>

      <section>
        <h2>9. Roadmap to Larger Games</h2>
        <ol>
          <li>
            <strong>Scheduler extensions.</strong> The discrete meta-regret module
            is ready to expose the scheduler to more than two strategies, essential
            when branching factors explode.
          </li>
          <li>
            <strong>Environment scaling.</strong> The Rust backend already handles
            stress runs with 12× more information states than the poker benchmarks,
            confirming memory usage and throughput stay stable in bigger domains.
          </li>
          <li>
            <strong>3D partial observability.</strong> New wrappers translate VizDoom
            and MineRL observations into the same latent space, letting ARMAC++
            practise on FPS and voxel environments without rewriting the learner.
          </li>
          <li>
            <strong>Baseline consistency.</strong> CFR anchors remain accurate as
            episode counts grow, keeping evaluation trustworthy for broader partially
            observed settings.
          </li>
          <li>
            <strong>Targeted improvements.</strong> Leduc exposes the same weaknesses
            —critic accuracy, exploration schedules, richer regret targets—that must
            be solved for larger imperfect-information games, so the research agenda
            transfers directly.
          </li>
        </ol>
      </section>

      <section>
        <h2>10. Candidate Games for Expansion</h2>
        <ul>
          <li>
            <strong>VizDoom deathmatch / navigation.</strong> Tests scheduler-driven
            policy mixing under dense visual observations and adversarial opponents.
          </li>
          <li>
            <strong>MineRL (Minecraft survival & tasks).</strong> Long-horizon,
            sparse reward problems that stress regret accumulation and actor
            agility; the CPU dataflow already ingests MineRL trajectories.
          </li>
          <li>
            <strong>Stratego-scale fog-of-war strategy.</strong> Hidden unit
            identities and deep trees stress the scheduler while matching the current
            observation encoding pattern.
          </li>
          <li>
            <strong>Diplomacy-style negotiation.</strong> Simultaneous orders and
            alliance commitments test dual-learning in fragile communication
            settings.
          </li>
          <li>
            <strong>MicroRTS with fog-of-war.</strong> Real-time action streams and
            partial observability benefit from Rust throughput and scheduler-
            controlled mixing.
          </li>
          <li>
            <strong>Security-resource allocation games.</strong> Stackelberg security
            domains (airport screening, network defense) use the same regret
            foundations but with far larger state spaces.
          </li>
        </ul>
        <p>
          Each of these domains already maps to the Rust environment trait;
          integrating them is a matter of supplying game-specific transition logic
          and observation encoders.
        </p>
      </section>

      <section>
        <h2>11. Immediate Next Steps</h2>
        <ul>
          <li>
            Integrate the discrete scheduler bins and the meta-regret manager so the
            mixing policy can shift among multiple strategies.
          </li>
          <li>
            Trial deeper or ensemble critics, starting with the heavier imperfect-
            information variants.
          </li>
          <li>
            Publish the throughput comparison between the Rust and Python backends to
            document the speed gains formally.
          </li>
          <li>
            Ship VizDoom and MineRL environment bindings so FPS and voxel tasks join
            the main experiment rotation alongside card games.
          </li>
        </ul>
      </section>

      <footer>
        ARMAC++ now delivers reproducible self-play, a trained scheduler, and a Rust
        backend that keeps iteration fast. With the environment layer validated on
        high-capacity stress runs, the project is positioned to move from small
        poker into VizDoom arenas, Minecraft survival, and the broader landscape of
        imperfect-information games.
      </footer>
    </main>
  </body>
</html>
