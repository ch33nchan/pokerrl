"""
Calculates the exact exploitability of a strategy in Kuhn Poker.

Exploitability is defined as the value a perfect best-response strategy can
achieve against the given strategy, averaged over all possible starting hands.
In a zero-sum game, a perfect Nash equilibrium strategy has an exploitability of 0.

The calculation involves two main steps:
1. For each player position, compute the best response strategy against the opponent's
   fixed strategy (the one being evaluated).
2. Calculate the expected value of playing the best response against the fixed strategy,
   averaging over all possible card deals.
"""

import numpy as np
from itertools import permutations
from ..environments.kuhn_poker import KuhnPoker

def calculate_exploitability(game: KuhnPoker, agent_wrapper) -> float:
    """
    Computes the exploitability of the agent's strategy in Kuhn Poker.

    Args:
        game: An instance of the KuhnPoker environment.
        agent_wrapper: A wrapper for the agent being evaluated, providing a
                       `get_action_probabilities(state)` method.

    Returns:
        The exploitability value in milli-big-blinds per hand (mbb/hand),
        though for Kuhn, it's just the raw expected value.
    """
    expected_value = 0
    
    # There are P(3,2) = 6 possible deals
    all_deals = list(permutations(range(game.NUM_CARDS), 2))
    
    for cards in all_deals:
        # Best response as player 0
        state_p0 = {"cards": cards, "history": "", "player": 0, "terminal": False}
        ev_p0 = _best_response_value(game, state_p0, 0, agent_wrapper)
        
        # Best response as player 1
        state_p1 = {"cards": cards, "history": "", "player": 0, "terminal": False}
        ev_p1 = -_best_response_value(game, state_p1, 1, agent_wrapper)
        
        expected_value += ev_p0 + ev_p1

    # Average over all deals and the two player positions
    return (expected_value / len(all_deals)) / 2


def _best_response_value(game: KuhnPoker, state: dict, br_player: int, agent_wrapper) -> float:
    """
    Recursively calculates the value of the best response strategy against the
    agent's fixed policy from a given state.
    """
    if game.is_terminal(state):
        return game.get_payoff(state, br_player)

    current_player = game.get_current_player(state)
    
    if current_player == br_player:
        # If it's the best-response player's turn, they choose the action that maximizes their value.
        action_values = []
        for action in game.get_legal_actions(state):
            next_state = game.get_next_state(state, action)
            action_values.append(_best_response_value(game, next_state, br_player, agent_wrapper))
        return max(action_values)
    else:
        # If it's the fixed agent's turn, we average over their strategy.
        strategy = agent_wrapper.get_action_probabilities(state)
        node_value = 0
        for action in game.get_legal_actions(state):
            if strategy[action] > 0:
                next_state = game.get_next_state(state, action)
                node_value += strategy[action] * _best_response_value(game, next_state, br_player, agent_wrapper)
        return node_value
