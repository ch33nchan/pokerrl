"""Exploitability calculation utilities for Leduc Hold'em policies."""

from __future__ import annotations

import os
import sys
import time
from itertools import permutations
from typing import Dict, Iterable, Tuple

import numpy as np
import torch

from qagent.agents.dcfr_leduc import DCFRTrainer
from qagent.encoders import FlatInfoSetEncoder
from qagent.environments.leduc_holdem import LeducEnv

MODEL_PATH = "leduc_dcfr_strategy_net.pt"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class PolicyWrapper:
    """Adapter exposing `get_action_probabilities()` for exploitability traversal."""

    def __init__(self, trainer: DCFRTrainer) -> None:
        self.trainer = trainer
        self.encoder = FlatInfoSetEncoder()

    def get_action_probabilities(self, state: Dict[str, object]) -> np.ndarray:
        infoset = self.trainer._build_infoset(state)
        mask = self._legal_action_mask(state)

        if hasattr(self.trainer, "_average_strategy_from_tensor"):
            info_tensor = self.encoder.encode_infoset(self.trainer.game, infoset).unsqueeze(0)
            strategy = self.trainer._average_strategy_from_tensor(info_tensor, mask)
        elif hasattr(self.trainer, "get_average_strategy") and hasattr(self.trainer, "_encode_info_set"):
            legal_actions = self.trainer.game.get_legal_actions(state)
            inputs = self.trainer._encode_info_set(state)
            strategy = self.trainer.get_average_strategy(inputs, legal_actions)
            if strategy.shape[0] != self.trainer.num_actions:
                full = np.zeros(self.trainer.num_actions, dtype=np.float32)
                for idx, action in enumerate(legal_actions):
                    full[action] = strategy[idx]
                strategy = full
        else:
            raise AttributeError(
                "Trainer must implement either `_average_strategy_from_tensor` or `get_average_strategy`."
            )

        return np.asarray(strategy, dtype=np.float32)

    def _legal_action_mask(self, state: Dict[str, object]) -> np.ndarray:
        mask = np.zeros(self.trainer.num_actions, dtype=np.float32)
        for action in self.trainer.game.get_legal_actions(state):
            mask[action] = 1.0
        return mask


class BestResponse:
    """Exact best-response evaluator for a fixed policy in Leduc."""

    def __init__(self, game: LeducEnv, policy: PolicyWrapper) -> None:
        self.game = game
        self.policy = policy

    def value(self, root_state: Dict[str, object], br_player: int) -> float:
        stack = [(root_state, "entry")]
        values: Dict[str, float] = {}

        while stack:
            state, phase = stack.pop()
            key = self.game.get_state_string(state)

            if self.game.is_terminal(state):
                values[key] = self.game.get_payoff(state, br_player)
                continue

            if phase == "entry":
                stack.append((state, "exit"))
                if self.game.is_chance_node(state):
                    for _, child in self._chance_successors(state):
                        if self.game.get_state_string(child) not in values:
                            stack.append((child, "entry"))
                else:
                    for action in self.game.get_legal_actions(state):
                        child = self.game.get_next_state(state, action)
                        if self.game.get_state_string(child) not in values:
                            stack.append((child, "entry"))
                continue

            if self.game.is_chance_node(state):
                total = 0.0
                for prob, child in self._chance_successors(state):
                    total += prob * values[self.game.get_state_string(child)]
                values[key] = total
                continue

            player = self.game.get_current_player(state)
            legal_actions = self.game.get_legal_actions(state)
            if player == br_player:
                best = -np.inf
                for action in legal_actions:
                    child = self.game.get_next_state(state, action)
                    value = values[self.game.get_state_string(child)]
                    best = max(best, value)
                values[key] = best
            else:
                probs = self.policy.get_action_probabilities(state)
                ev = 0.0
                for action in legal_actions:
                    prob = probs[action]
                    if prob <= 0:
                        continue
                    child = self.game.get_next_state(state, action)
                    ev += prob * values[self.game.get_state_string(child)]
                values[key] = ev

        return values[self.game.get_state_string(root_state)]

    def _chance_successors(self, state: Dict[str, object]) -> Iterable[Tuple[float, Dict[str, object]]]:
        chance = self.game.sample_chance_outcome(state)
        if isinstance(chance, list):
            return chance
        if isinstance(chance, tuple) and len(chance) == 2:
            return [chance]
        raise RuntimeError("Unexpected chance outcome format")


def calculate_exploitability(trainer: DCFRTrainer) -> float:
    game: LeducEnv = trainer.game if isinstance(trainer.game, LeducEnv) else LeducEnv()
    policy = PolicyWrapper(trainer)
    br = BestResponse(game, policy)

    deck = list(range(game.deck_size))
    total_value = 0.0
    deals = 0

    print("Calculating exploitability over all possible initial deals...")
    start_time = time.time()

    for cards in permutations(deck, 2):
        deals += 1
        state = game.get_initial_state()
        state["private_cards"] = list(cards)
        state["deck"] = [c for c in deck if c not in cards]

        value_p0 = br.value(state, 0)
        flipped = state.copy()
        flipped["private_cards"] = [cards[1], cards[0]]
        value_p1 = br.value(flipped, 1)
        total_value += (value_p0 + value_p1) / 2.0

    exploitability = total_value / deals
    elapsed = time.time() - start_time
    print(f"Calculation finished in {elapsed:.2f} seconds.")
    print(f"Exploitability (game value): {exploitability:.6f}")
    return exploitability


def calculate_nash_conv_approx(trainer: DCFRTrainer) -> float:
    """Approximate NashConv using symmetric best responses.

    For two-player zero-sum poker, a common proxy for NashConv is the average
    of both players' best-response values against the evaluated policy.

    This function mirrors the enumeration in `calculate_exploitability` but
    returns a value directly interpretable as a NashConv-style quantity.

    Notes:
    - Exact NashConv requires the environment's payoff normalization to match
      OpenSpiel. This routine is used for apples-to-apples comparison with our
      in-house evaluator; OpenSpiel baselines should be used as the reference
      standard via `qagent.evaluation.openspiel_baselines`.
    """
    game: LeducEnv = trainer.game if isinstance(trainer.game, LeducEnv) else LeducEnv()
    policy = PolicyWrapper(trainer)
    br = BestResponse(game, policy)

    deck = list(range(game.deck_size))
    total = 0.0
    deals = 0
    for cards in permutations(deck, 2):
        deals += 1
        state = game.get_initial_state()
        state["private_cards"] = list(cards)
        state["deck"] = [c for c in deck if c not in cards]
        v0 = br.value(state, 0)

        flipped = state.copy()
        flipped["private_cards"] = [cards[1], cards[0]]
        v1 = br.value(flipped, 1)
        total += 0.5 * (v0 + v1)
    return total / max(deals, 1)


def load_trainer(path: str, game: LeducEnv | None = None) -> DCFRTrainer:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Model file not found at '{path}'. Train a policy first.")
    game = game or LeducEnv()
    trainer = DCFRTrainer(game=game)
    state_dict = torch.load(path, map_location=DEVICE)
    trainer.strategy_net.load_state_dict(state_dict)
    return trainer


def main() -> None:
    trainer = load_trainer(MODEL_PATH)
    calculate_exploitability(trainer)


if __name__ == "__main__":
    main()

